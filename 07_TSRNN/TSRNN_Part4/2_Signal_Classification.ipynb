{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Signal Classification"
      ],
      "metadata": {
        "id": "82dhsvRUKofa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "訊號與圖的分類一樣，在preprocess後可使用神經網路做一些AI任務，例如訊號分類、迴歸還有生成等等。\n",
        "\n",
        "這個部分我們用音訊作為訊號的範例，來試著將聲音訊號做分類，包含以下部分:\n",
        "- Audio Data Loader\n",
        "- RNN audio classification\n",
        "- CNN audio classification"
      ],
      "metadata": {
        "id": "5mo7TK6mKrry"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "開始之前我們先準備一些內容。\n",
        "\n",
        "我們使用的範例資料集是tensorflow提供的[Mini Speech Commands](https://ai.googleblog.com/2017/08/launching-speech-commands-dataset.html)資料集，從官網下載。"
      ],
      "metadata": {
        "id": "R-B1OWAKMDtL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jTGJO2yTGgQ_"
      },
      "outputs": [],
      "source": [
        "!wget http://storage.googleapis.com/download.tensorflow.org/data/mini_speech_commands.zip\n",
        "!unzip mini_speech_commands.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "import IPython.display as idp # 播音工具\n",
        "import librosa.display as ldp # 畫頻譜圖工具\n",
        "import numpy as np # 輔助運算\n",
        "import matplotlib.pyplot as plt # 輔助畫圖\n",
        "\n",
        "# import model用到的內容\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "g1m6erWYMR6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Audio Data Loader"
      ],
      "metadata": {
        "id": "jo_9jVjhRVfL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "與前面CNN相同，需要有data loader去對資料做讀取，而tensorflow沒有原生讀音訊的data loader (TF 2.11有，但在Colab上要另外灌TF2.11)，所以這部分要自己寫。"
      ],
      "metadata": {
        "id": "ryzrWNvWRZqU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 讀取音訊檔及資料切分"
      ],
      "metadata": {
        "id": "KPMvRiOT5uiQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from glob import glob #拿來列資料夾內容的小套件\n",
        "from sklearn.model_selection import train_test_split # 切分資料集用\n",
        "\n",
        "def find_class(x):\n",
        "    # 根據格式，找到所屬class\n",
        "    return x.strip('/')[-2]\n",
        "\n",
        "def audio_folder_datasets( dataset_path,class_dictionary, sr=22050, duration=-1):\n",
        "    # 輸入: \n",
        "    ## dataset_path - 資料夾，內有數個不同class的資料夾，內有.wav檔\n",
        "    ## class_dictionary - dictionary物件，對應每個資料夾的class\n",
        "    ## sr -  讀取的sampling rate\n",
        "    ## duration - 可指定秒數(float32)，不指定則為原檔長度\n",
        "    file_names = []\n",
        "    labels = []\n",
        "    for cls, class_id in class_dictionary.items():\n",
        "        f_list=glob(dataset_path+f'{cls}/*.wav') # 找到該class的所有檔案\n",
        "        file_names.extend(f_list) # 加入列表\n",
        "        labels.extend([class_id]*len(f_list)) # 加入相應labels\n",
        "    print(\"total:\",f\"{len(file_names)} files of {len(class_dictionary)} classes\")\n",
        "    \n",
        "    @tf.function # 套tf.function使其可以對tensorflow tensor做動\n",
        "    def load_wav(fname):\n",
        "        # 使用指定sampling rate, duration讀檔\n",
        "        # 這邊要用到librosa的loading才有re-sample，若已經知道每個檔案sampling rate也可以用tensorflow的tf.audio\n",
        "        # 可參考: https://www.kaggle.com/code/lkergalipatak/bird-audio-classification-with-tensorflow\n",
        "\n",
        "        x= tf.numpy_function(\n",
        "            lambda x: librosa.util.fix_length(\n",
        "                librosa.load(x, sr=sr, duration=duration)[0],\n",
        "                size=int(sr*duration),\n",
        "                mode='edge'),\n",
        "            inp=[fname], Tout=tf.float32)\n",
        "        return x\n",
        "\n",
        "    def get_dataset(paths_,labels_):\n",
        "        # 得到所有資料夾名稱\n",
        "        path_ds = tf.data.Dataset.from_tensor_slices(paths_) # 轉換成檔名的Dataset物件\n",
        "        label_ds= tf.data.Dataset.from_tensor_slices(labels_)\n",
        "\n",
        "        data_ds = path_ds.map(\n",
        "                load_wav,\n",
        "                num_parallel_calls=tf.data.AUTOTUNE,\n",
        "            )\n",
        "        return tf.data.Dataset.zip((data_ds,label_ds))\n",
        "\n",
        "    fname_train, fname_val, label_train, label_val = train_test_split(file_names,labels,test_size=0.2)\n",
        "    return get_dataset(fname_train,label_train), get_dataset(fname_val,label_val)\n",
        "    "
      ],
      "metadata": {
        "id": "m3KXFuxQRZD7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "生成一個dataset拿來用作基礎。"
      ],
      "metadata": {
        "id": "DfRAtBnZ2ZIu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 準備一些參數輸入進function生成dataset\n",
        "DATASET_PATH='mini_speech_commands/'\n",
        "class_dict = {\n",
        "    'down':0,\n",
        "    'go':1,\n",
        "    'left':2,\n",
        "    'no':3,\n",
        "    'right':4,\n",
        "    'stop':5,\n",
        "    'up':6,\n",
        "    'yes':7\n",
        "}\n",
        "SR = 22050\n",
        "DURATION = 0.8\n",
        "tran_ds, val_ds = audio_folder_datasets(DATASET_PATH, class_dict, sr=SR, duration=DURATION)"
      ],
      "metadata": {
        "id": "Yow6YjV7eqcY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "dataset一次丟出一個signal以及一個label\n",
        "\n",
        "觀察data基本性質"
      ],
      "metadata": {
        "id": "iRVQN_8r3tZw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for x,y in tran_ds:\n",
        "    print('\\n',x.shape,x.numpy().min(),x.numpy().max())\n",
        "    print(y)\n",
        "    break\n",
        "# 畫出來\n",
        "ldp.waveplot(x.numpy().squeeze(), sr=SR)\n",
        "\n",
        "# 聽看看\n",
        "idp.Audio(x.numpy().squeeze(), rate=SR)"
      ],
      "metadata": {
        "id": "jzc53mLj2Q4W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "畫出一個例子"
      ],
      "metadata": {
        "id": "SDrINt5U3xh2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 輸入NN前做轉換"
      ],
      "metadata": {
        "id": "hb0sjjxy50he"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "在預處裡時使用librosa會很慢，幸好與```librosa.stft```類似，可以使用```tensorflow.signal.stft```做時頻分析，在操作的各種過程中只能用tf function來操作。\n",
        "\n",
        "其axis為[time,frequency]，與librosa相反，使用```librosa.specshow```觀察時記得要做transpose。\n",
        "\n",
        "為什麼要反過來是為了配合RNN的預設axis [batch,time,...]，將time擺在batch後面第一位。\n",
        "\n",
        "若希望使用CNN模型，記得在最後多加一個空的axis，因為CNN適用的axis是[batch, hight, width, channels]。"
      ],
      "metadata": {
        "id": "5IZGg4NvV1s-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N=512\n",
        "H=128\n",
        "def get_stft(waveform):\n",
        "    # 做STFT (用tensorflow得比較快)\n",
        "    spectrogram = tf.signal.stft(\n",
        "        waveform, frame_length=N, frame_step=H)\n",
        "    # 這邊frame_length是librosa的n_fft\n",
        "    #     frame_step是librosa的hop_length\n",
        "    # 使用tf.signal stft出來時，單位為((timepoints-n_fft)/hop_length, n_fft/2)\n",
        "    # 這是為了配合RNN等模型的\n",
        "\n",
        "    # 取magnitude\n",
        "    spectrogram = tf.abs(spectrogram)\n",
        "\n",
        "    # 若是多加一個維度，可以用於CNN，shape (`batch_size`, `height`, `width`, `channels`).\n",
        "    # spectrogram = spectrogram[..., tf.newaxis]\n",
        "    return spectrogram\n",
        "\n",
        "# 使用STFT當作preprocess function\n",
        "tran_ds_stft= tran_ds.map(lambda x,y: (get_stft(x), y), num_parallel_calls=tf.data.AUTOTUNE).cache().shuffle(6400).prefetch(tf.data.AUTOTUNE)\n",
        "val_ds_stft= val_ds.map(lambda x,y: (get_stft(x), y), num_parallel_calls=tf.data.AUTOTUNE).cache().prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "jLi5zn7a3NQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 可以看一下資料\n",
        "for x_S,y in val_ds_stft:\n",
        "    print(x_S.shape,x_S.numpy().min(),x_S.numpy().max())\n",
        "    print(y)\n",
        "    break\n",
        "plt.figure(figsize=(6,5))\n",
        "ldp.specshow(x_S.numpy().T,sr=SR,x_axis=\"s\",y_axis=\"hz\",cmap=\"jet\") # 記得做transpose\n",
        "plt.colorbar(format=\"%+4.f\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CBWPxPyWW08V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 跟librosa差不多，librosa有做time padding，會長一些\n",
        "x_S_ = librosa.stft(x.numpy(), n_fft=N, hop_length=H)\n",
        "plt.figure(figsize=(6,5))\n",
        "ldp.specshow(abs(x_S_), sr=SR, x_axis=\"s\", y_axis=\"hz\", cmap=\"jet\")\n",
        "plt.colorbar(format=\"%+4.f\")\n",
        "plt.show()\n",
        "x_S_.shape"
      ],
      "metadata": {
        "id": "fhfraNwUTo6X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# 預讀資料，放進GPU\n",
        "for x_S,y in tran_ds_stft:\n",
        "    pass"
      ],
      "metadata": {
        "id": "VxHlfYUynEEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "這邊也可以進一步使用MFCC，但因為```tensorflow.signal```做MFCC的步驟過於複雜且須配合的指令太多，這邊我們使用librosa套tf.function做轉換\n",
        "\n",
        "若想嘗試做tensorflow的版本可參考: https://github.com/timsainb/tensorflow2-generative-models/blob/master/7.0-Tensorflow-spectrograms-and-inversion.ipynb"
      ],
      "metadata": {
        "id": "efrNH2Pwqj_f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N_MFCC=39\n",
        "\n",
        "# 立一個方便的function給MFCC，套用想要的arguments，這邊記得，librosa做出來給tensorflow時需要做transpose\n",
        "def mfcc_function(waveform):\n",
        "    return librosa.feature.mfcc(y=waveform,\n",
        "                                sr=SR,\n",
        "                                n_mfcc=N_MFCC,\n",
        "                                n_fft=N,\n",
        "                                hop_length=H).T\n",
        "@tf.function\n",
        "def get_mfcc(waveform):\n",
        "    # 做MFCC，因為用librosa所以要用numpy_function包起來\n",
        "    mfcc = tf.numpy_function(mfcc_function, [waveform], tf.float32)\n",
        "    # 若是多加一個維度，可以用於CNN，shape (`batch_size`, `height`, `width`, `channels`).\n",
        "    # mfcc = mfcc[..., tf.newaxis]\n",
        "    return mfcc\n",
        "\n",
        "# 使用STFT當作preprocess function\n",
        "tran_ds_mfcc= tran_ds.map(lambda x,y: (get_mfcc(x), y), num_parallel_calls=tf.data.AUTOTUNE).cache().shuffle(6400).prefetch(tf.data.AUTOTUNE)\n",
        "val_ds_mfcc= val_ds.map(lambda x,y: (get_mfcc(x), y), num_parallel_calls=tf.data.AUTOTUNE).cache().prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "MvTZ2OFlqGFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 可以看一下資料\n",
        "for x_C,y in tran_ds_mfcc:\n",
        "    print(x_C.shape,x_C.numpy().min(),x_C.numpy().max())\n",
        "    print(y)\n",
        "    break\n",
        "plt.figure(figsize=(6,5))\n",
        "ldp.specshow(x_C.numpy().T,sr=SR,x_axis=\"s\",cmap=\"jet\") # 記得做transpose\n",
        "plt.colorbar(format=\"%+4.f\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cLyK1UQhw8sq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# 預讀資料，放進GPU\n",
        "for x_C,y in tran_ds_mfcc:\n",
        "    pass"
      ],
      "metadata": {
        "id": "b0yeRNxYzS2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RNN Audio classifcation"
      ],
      "metadata": {
        "id": "4Im2kjcGmFSM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### STFT preprocess + RNN"
      ],
      "metadata": {
        "id": "l938Da7XyWNS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "我們可使用RNN來做對剛剛的頻譜作classification的訓練"
      ],
      "metadata": {
        "id": "UyWP60TbmKoJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 抓一下 data的大小\n",
        "for example_spectrograms, example_spect_labels in tran_ds_stft.take(1):\n",
        "  break\n",
        "input_shape = example_spectrograms.shape.as_list()"
      ],
      "metadata": {
        "id": "je3BLdd3mB31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import models"
      ],
      "metadata": {
        "id": "krVVzebD5q2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tf.keras.Input(shape=(None,input_shape[1]))\n",
        "h = layers.LSTM(256, dropout=0.1)(inputs) # 用層LSTM\n",
        "outputs = layers.Dense(len(class_dict),activation='softmax')(h)\n",
        "\n",
        "model = models.Model(inputs=inputs, outputs=outputs)"
      ],
      "metadata": {
        "id": "qfgu4V4FZBuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy'],\n",
        ")"
      ],
      "metadata": {
        "id": "SN25M3uCc-99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 20\n",
        "history = model.fit(\n",
        "    tran_ds_stft.batch(32),\n",
        "    validation_data=val_ds_stft.batch(64),\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=tf.keras.callbacks.EarlyStopping(verbose=1, patience=2),\n",
        ")"
      ],
      "metadata": {
        "id": "XFA7WMUfdXe0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(val_ds_stft.batch(64))"
      ],
      "metadata": {
        "id": "iyJf2KFvhF2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MFCC preprocess + RNN"
      ],
      "metadata": {
        "id": "8YUFmCceyfyS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 抓一下 data的大小\n",
        "for example_cepstrums, example_spect_labels in tran_ds_mfcc.take(1):\n",
        "  break\n",
        "input_shape = example_cepstrums.shape.as_list()\n",
        "\n",
        "inputs = tf.keras.Input(shape=(None,input_shape[1])) # 直接使用剛剛抓的大小\n",
        "h = layers.LSTM(256, dropout=0.1)(inputs) # 用層LSTM\n",
        "outputs = layers.Dense(len(class_dict),activation='softmax')(h)\n",
        "\n",
        "model = models.Model(inputs=inputs, outputs=outputs)"
      ],
      "metadata": {
        "id": "_cPxEoNkyw1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy'],\n",
        ")"
      ],
      "metadata": {
        "id": "F8rxfWpeyw1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 20\n",
        "history = model.fit(\n",
        "    tran_ds_mfcc.batch(32),\n",
        "    validation_data=val_ds_mfcc.batch(64),\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=tf.keras.callbacks.EarlyStopping(verbose=1, patience=2),\n",
        ")"
      ],
      "metadata": {
        "id": "prLl9wsmyw1l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(val_ds_mfcc.batch(64))"
      ],
      "metadata": {
        "id": "Qqq3cgMlyw1l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "可以自行嘗試fine-tune這兩種結果。\n",
        "\n",
        "近期語音AI論文最常用的preprocess方式可作為訊號處理AI的前處理參考:\n",
        "1. 直接用一組Filter Bank\n",
        "2. MFCC\n",
        "3. Spectrogram\n",
        "\n",
        "(參考: 李宏毅老師Deep Learning for Human Language Processing (2020,Spring)課程中助教統計https://youtu.be/AIKu43goh-8?list=PLJV_el3uVTsO07RpBYFsXg-bN5Lu0nhdG&t=1964)"
      ],
      "metadata": {
        "id": "ecEwkyJ515pT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CNN Audio Classification"
      ],
      "metadata": {
        "id": "abp-h8Y4FeqB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "當然，因為我們已經把資料轉換成2D的頻譜了，所以也可以當作一張圖來做2D CNN。\n",
        "\n",
        "這邊用的是STFT做時頻轉換的結果。"
      ],
      "metadata": {
        "id": "DGxmSwjJOfUL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def extend_dims(x, y):\n",
        "    return x[...,np.newaxis], y\n",
        "tran_ds_stft_ = tran_ds_stft.map(extend_dims)\n",
        "val_ds_stft_ = val_ds_stft.map(extend_dims)"
      ],
      "metadata": {
        "id": "Od6y17AQG32M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tf.keras.Input(shape=(None,input_shape[1],1))\n",
        "h = layers.Conv2D(32,(3,3), activation='relu')(inputs)\n",
        "h = layers.Dropout(0.1)(h)\n",
        "h = layers.MaxPooling2D()(h)\n",
        "h = layers.Conv2D(64,(3,3), activation='relu')(h)\n",
        "h = layers.Dropout(0.1)(h)\n",
        "h = layers.MaxPooling2D()(h)\n",
        "h = layers.Conv2D(64,(3,3), activation='relu')(h)\n",
        "h = layers.Dropout(0.1)(h)\n",
        "h = layers.GlobalAveragePooling2D()(h)\n",
        "h = layers.Flatten()(h)\n",
        "h = layers.Dense(32)(h)\n",
        "outputs = layers.Dense(len(class_dict),activation='softmax')(h)\n",
        "\n",
        "model = models.Model(inputs=inputs, outputs=outputs)"
      ],
      "metadata": {
        "id": "cspH98fnHqQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy'],\n",
        ")"
      ],
      "metadata": {
        "id": "zOBrKssMIzLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 20\n",
        "history = model.fit(\n",
        "    tran_ds_stft_.batch(32),\n",
        "    validation_data=val_ds_stft_.batch(64),\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=tf.keras.callbacks.EarlyStopping(verbose=1, patience=2),\n",
        ")"
      ],
      "metadata": {
        "id": "YrtK2_H5IzLe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(val_ds_stft_.batch(64))"
      ],
      "metadata": {
        "id": "okm8WD97OZYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "使用CNN的好處是，已經有很多CNN-based的pre-train模型可以使用來做transfer learning。\n",
        "\n",
        "建議也可以拿transfer leraning提及的model來訓練看看!\n",
        "\n",
        "亦可嘗試使用MFCC Preprocess後做CNN模型。"
      ],
      "metadata": {
        "id": "W2VOtUg8Oq55"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reference\n",
        "* TF官網教學: https://www.tensorflow.org/tutorials/audio/simple_audio\n",
        "* https://towardsdatascience.com/audio-augmentations-in-tensorflow-48483260b169\n",
        "* https://github.com/timsainb/tensorflow2-generative-models/blob/master/7.0-Tensorflow-spectrograms-and-inversion.ipynb"
      ],
      "metadata": {
        "id": "x1oWIACng2wH"
      }
    }
  ]
}