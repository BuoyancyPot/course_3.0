{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dee71487",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **Intro to common CNN APIs**\n",
    "此份程式碼會介紹在 CNN model 當中常使用的 Layers。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83777eb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 本章節大綱\n",
    "* [Conv2D( filters, kernel_size, strides, use_bias)](#Conv2D)\n",
    "  * [use_bias](#use-bias)\n",
    "  * [Multi-Channels](#Multi-Channels-with-1-Filter)\n",
    "  * [filters](#filters)\n",
    "  * [kernel_size](#kernel-_-size)\n",
    "  * [strides](#strides)\n",
    "* [Flatten](#Flatten)\n",
    "* [Padding](#Padding)\n",
    "* [Pooling](#Pooling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56995fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d08b2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下載並解壓縮課程所需檔案\n",
    "!wget -q \"https://github.com/TA-aiacademy/course_3.0/releases/download/CVCNN_Data/cnn_part2_data.zip\"\n",
    "!unzip -q cnn_part2_data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b4043a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = np.array([[0, 0, 0, 0, 0, 0],\n",
    "                      [0, 0, 0, 1, 1, 0],\n",
    "                      [0, 1, 1, 1, 1, 0],\n",
    "                      [0, 0, 1, 0, 1, 0],\n",
    "                      [0, 0, 0, 1, 0, 0],\n",
    "                      [0, 0, 0, 0, 0, 0]], dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ae93354",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 6)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bb13eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 6, 6, 1)\n",
      "(batch_size, height, width, channel)\n"
     ]
    }
   ],
   "source": [
    "input_img = input_img[np.newaxis, ..., np.newaxis]\n",
    "print(input_img.shape)\n",
    "print(\"(batch_size, height, width, channel)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0bd83b",
   "metadata": {},
   "source": [
    "* ## Conv2D\n",
    "![](https://i.imgur.com/ziscEhS.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b36ff661",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_init(shape, dtype=None):\n",
    "    filter_init = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]], dtype='float32')\n",
    "    # height, width, channel, filters\n",
    "    filter_init = filter_init.reshape((3, 3, 1, 1))\n",
    "    return tf.Variable(filter_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab6b234b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-08 15:27:50.084893: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-08 15:27:50.765821: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10417 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:04:00.0, compute capability: 6.1\n",
      "2023-03-08 15:27:51.237649: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8101\n"
     ]
    }
   ],
   "source": [
    "conv_result = Conv2D(filters=1, kernel_size=(3, 3), strides=(1, 1),\n",
    "                     kernel_initializer=kernel_init)(input_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "262cb577",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_result = conv_result.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "472dbb4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 4, 4, 1)\n",
      "[[1. 1. 2. 1.]\n",
      " [2. 1. 2. 2.]\n",
      " [0. 3. 1. 2.]\n",
      " [0. 0. 2. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(conv_result.shape)\n",
    "print(conv_result.squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c8ea34",
   "metadata": {},
   "source": [
    "[(back...)](#Convolution2D)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8fd2a0",
   "metadata": {},
   "source": [
    "* ## use bias\n",
    "![](https://i.imgur.com/3x4wMGO.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8f8e536",
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_result = Conv2D(filters=1, kernel_size=(3, 3), strides=(1, 1),\n",
    "                     kernel_initializer=kernel_init,\n",
    "                     use_bias=True,\n",
    "                     bias_initializer='ones')(input_img)\n",
    "\n",
    "bias_result = bias_result.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1107cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 4, 4, 1)\n",
      "[[2. 2. 3. 2.]\n",
      " [3. 2. 3. 3.]\n",
      " [1. 4. 2. 3.]\n",
      " [1. 1. 3. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(bias_result.shape)\n",
    "print(bias_result.squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f327cff",
   "metadata": {},
   "source": [
    "[(back...)](#Convolution2D)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47813940",
   "metadata": {},
   "source": [
    "* ## Multi Channels with 1 Filter\n",
    "![](https://i.imgur.com/NCivRaq.gif)\n",
    "![](https://i.imgur.com/QEjI0jq.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fcd45c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 6, 3)\n",
      "int64\n"
     ]
    }
   ],
   "source": [
    "input_img = np.load(\"./data/conv2d_multichannel_input.npy\")\n",
    "print(input_img.shape)\n",
    "print(input_img.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "875e58bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 6, 6, 3)\n",
      "(Batch_size, Height, Width, Channel)\n"
     ]
    }
   ],
   "source": [
    "input_img = input_img[np.newaxis, ...]\n",
    "print(input_img.shape)\n",
    "print(\"(Batch_size, Height, Width, Channel)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "968e0aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32\n"
     ]
    }
   ],
   "source": [
    "input_img = input_img.astype(\"float32\")\n",
    "print(input_img.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a2d7093a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3, 3, 1)\n",
      "(Height, Width, Channel, Num of Filters)\n"
     ]
    }
   ],
   "source": [
    "filter_init = np.load(\"./data/conv2d_multichannelfilter.npy\")\n",
    "print(filter_init.shape)\n",
    "print(\"(Height, Width, Channel, Num of Filters)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "edfacecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_init = tf.constant_initializer(filter_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "739aed31",
   "metadata": {},
   "outputs": [],
   "source": [
    "multichannel = Conv2D(filters=1, kernel_size=(3, 3), strides=(1, 1),\n",
    "                      kernel_initializer=kernel_init)(input_img)\n",
    "\n",
    "multichannel = multichannel.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a6c85a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 4, 4, 1)\n",
      "[[2. 3. 5. 3.]\n",
      " [5. 3. 5. 5.]\n",
      " [3. 4. 5. 6.]\n",
      " [1. 2. 5. 2.]]\n"
     ]
    }
   ],
   "source": [
    "print(multichannel.shape)\n",
    "print(multichannel.squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c56db88",
   "metadata": {},
   "source": [
    "[(back...)](#Convolution2D)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef573b9",
   "metadata": {},
   "source": [
    "* ## filters\n",
    "![](https://i.imgur.com/NCivRaq.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f1a721e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3, 3, 8)\n"
     ]
    }
   ],
   "source": [
    "multi_filter_init = np.zeros((3, 3, 3, 8))\n",
    "for i in range(8):\n",
    "    multi_filter_init[:, :, :, i] = filter_init.squeeze()\n",
    "multi_filter_init = multi_filter_init.astype('float32')\n",
    "\n",
    "print(multi_filter_init.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c60974d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_init = tf.constant_initializer(multi_filter_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "984b895c",
   "metadata": {},
   "outputs": [],
   "source": [
    "multifilter = Conv2D(8, (3, 3), strides=(1, 1),\n",
    "                     kernel_initializer=kernel_init)(input_img)\n",
    "\n",
    "multifilter = multifilter.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2dbf173c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 4, 4, 8)\n",
      "[[[2. 2. 2. 2. 2. 2. 2. 2.]\n",
      "  [3. 3. 3. 3. 3. 3. 3. 3.]\n",
      "  [5. 5. 5. 5. 5. 5. 5. 5.]\n",
      "  [3. 3. 3. 3. 3. 3. 3. 3.]]\n",
      "\n",
      " [[5. 5. 5. 5. 5. 5. 5. 5.]\n",
      "  [3. 3. 3. 3. 3. 3. 3. 3.]\n",
      "  [5. 5. 5. 5. 5. 5. 5. 5.]\n",
      "  [5. 5. 5. 5. 5. 5. 5. 5.]]\n",
      "\n",
      " [[3. 3. 3. 3. 3. 3. 3. 3.]\n",
      "  [4. 4. 4. 4. 4. 4. 4. 4.]\n",
      "  [5. 5. 5. 5. 5. 5. 5. 5.]\n",
      "  [6. 6. 6. 6. 6. 6. 6. 6.]]\n",
      "\n",
      " [[1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "  [2. 2. 2. 2. 2. 2. 2. 2.]\n",
      "  [5. 5. 5. 5. 5. 5. 5. 5.]\n",
      "  [2. 2. 2. 2. 2. 2. 2. 2.]]]\n"
     ]
    }
   ],
   "source": [
    "print(multifilter.shape)\n",
    "print(multifilter.squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3f72a2",
   "metadata": {},
   "source": [
    "[(back...)](#Convolution2D)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669d0806",
   "metadata": {},
   "source": [
    "* ## strides\n",
    "![](https://i.imgur.com/8XWHNqI.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1a532da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = np.load(\"./data/conv2d_1channel_input.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "875b7282",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_init = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]], dtype='float32')\n",
    "filter_init = filter_init.reshape((3, 3, 1, 1))\n",
    "kernel_init = tf.constant_initializer(filter_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dd4ebaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "stride_result = Conv2D(1, (3, 3), strides=(2, 2),\n",
    "                       kernel_initializer=kernel_init)(input_img)\n",
    "\n",
    "stride_result = stride_result.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dbefd9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2, 2, 1)\n",
      "[[1. 2.]\n",
      " [0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(stride_result.shape)\n",
    "print(stride_result.squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19eb5024",
   "metadata": {},
   "source": [
    "![](https://i.imgur.com/2XmNAct.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccda958d",
   "metadata": {},
   "source": [
    "# Flatten\n",
    "\n",
    "* [Way1-Reshape](#Way1---Reshape)\n",
    "* [Way2-Flatten](#Way2---Flatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4be199e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.layers import Flatten, Reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aff2f2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img1 = np.array([[0, 1, 2, 3],\n",
    "                       [4, 5, 6, 7],\n",
    "                       [8, 9, 10, 11],\n",
    "                       [12, 13, 14, 15]], dtype='float32')\n",
    "input_img1 = input_img1[np.newaxis, ..., np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef1fe3d",
   "metadata": {},
   "source": [
    "* ## Way1 - Reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "07841bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape_result = Reshape(target_shape=(-1,))(input_img1)\n",
    "reshape_result = reshape_result.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f6ecd3cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 4, 4, 1)\n",
      "(1, 16)\n",
      "[[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15.]]\n"
     ]
    }
   ],
   "source": [
    "print(input_img1.shape)\n",
    "print(reshape_result.shape)\n",
    "print(reshape_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8dfab41c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 4, 4, 8)\n"
     ]
    }
   ],
   "source": [
    "input_img2 = input_img1.copy()\n",
    "for _ in range(3):\n",
    "    input_img2 = np.concatenate([input_img2, input_img2], -1)\n",
    "print(input_img2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8ec8b144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 128)\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  1.  1.  1.  1.  1.  1.  2.  2.\n",
      "   2.  2.  2.  2.  2.  2.  3.  3.  3.  3.  3.  3.  3.  3.  4.  4.  4.  4.\n",
      "   4.  4.  4.  4.  5.  5.  5.  5.  5.  5.  5.  5.  6.  6.  6.  6.  6.  6.\n",
      "   6.  6.  7.  7.  7.  7.  7.  7.  7.  7.  8.  8.  8.  8.  8.  8.  8.  8.\n",
      "   9.  9.  9.  9.  9.  9.  9.  9. 10. 10. 10. 10. 10. 10. 10. 10. 11. 11.\n",
      "  11. 11. 11. 11. 11. 11. 12. 12. 12. 12. 12. 12. 12. 12. 13. 13. 13. 13.\n",
      "  13. 13. 13. 13. 14. 14. 14. 14. 14. 14. 14. 14. 15. 15. 15. 15. 15. 15.\n",
      "  15. 15.]]\n"
     ]
    }
   ],
   "source": [
    "reshape_result = Reshape(target_shape=(-1,))(input_img2)\n",
    "reshape_result = reshape_result.numpy()\n",
    "print(reshape_result.shape)\n",
    "print(reshape_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d3d92a",
   "metadata": {},
   "source": [
    "[(back...)](#Flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920e1a52",
   "metadata": {},
   "source": [
    "* ## Way2 - Flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29926cf7",
   "metadata": {},
   "source": [
    "![](https://i.imgur.com/MvwO4a0.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7a3f9301",
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten_result = Flatten()(input_img1)\n",
    "flatten_result = flatten_result.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c3867c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 4, 4, 1)\n",
      "(1, 16)\n",
      "[[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15.]]\n"
     ]
    }
   ],
   "source": [
    "print(input_img1.shape)\n",
    "print(flatten_result.shape)\n",
    "print(flatten_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937fe696",
   "metadata": {},
   "source": [
    "![](https://i.imgur.com/FDh4d0L.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d029854a",
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten_result = Flatten()(input_img2)\n",
    "flatten_result = flatten_result.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "74790502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 128)\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  1.  1.  1.  1.  1.  1.  2.  2.\n",
      "   2.  2.  2.  2.  2.  2.  3.  3.  3.  3.  3.  3.  3.  3.  4.  4.  4.  4.\n",
      "   4.  4.  4.  4.  5.  5.  5.  5.  5.  5.  5.  5.  6.  6.  6.  6.  6.  6.\n",
      "   6.  6.  7.  7.  7.  7.  7.  7.  7.  7.  8.  8.  8.  8.  8.  8.  8.  8.\n",
      "   9.  9.  9.  9.  9.  9.  9.  9. 10. 10. 10. 10. 10. 10. 10. 10. 11. 11.\n",
      "  11. 11. 11. 11. 11. 11. 12. 12. 12. 12. 12. 12. 12. 12. 13. 13. 13. 13.\n",
      "  13. 13. 13. 13. 14. 14. 14. 14. 14. 14. 14. 14. 15. 15. 15. 15. 15. 15.\n",
      "  15. 15.]]\n"
     ]
    }
   ],
   "source": [
    "print(flatten_result.shape)\n",
    "print(flatten_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3f590a",
   "metadata": {},
   "source": [
    "[(back...)](#Flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b3d886",
   "metadata": {},
   "source": [
    "# Padding\n",
    "\n",
    "* [padding='VALID'](#padding='VALID')\n",
    "* [padding='SAME'](#padding='SAME')\n",
    "* [ZeroPadding](#ZeroPadding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "66d8804f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, ZeroPadding2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0c19cd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = np.array([[0, 0, 0, 0, 0, 0],\n",
    "                      [0, 0, 0, 1, 1, 0],\n",
    "                      [0, 1, 1, 1, 1, 0],\n",
    "                      [0, 0, 1, 0, 1, 0],\n",
    "                      [0, 0, 0, 1, 0, 0],\n",
    "                      [0, 0, 0, 0, 0, 0]], dtype='float32')\n",
    "input_img = input_img[np.newaxis, ..., np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "56c674c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_init(shape, dtype=None):\n",
    "    filter_init = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n",
    "    filter_init = filter_init.reshape((3, 3, 1, 1))\n",
    "    return tf.Variable(filter_init, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff09c0c5",
   "metadata": {},
   "source": [
    "* ## padding='VALID'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2ed09cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "nopad_result = Conv2D(1, (3, 3), padding='VALID',\n",
    "                      kernel_initializer=kernel_init)(input_img)\n",
    "\n",
    "nopad_result = nopad_result.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5650716d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 6, 6, 1)\n",
      "(1, 4, 4, 1)\n",
      "[[1. 1. 2. 1.]\n",
      " [2. 1. 2. 2.]\n",
      " [0. 3. 1. 2.]\n",
      " [0. 0. 2. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(input_img.shape)\n",
    "print(nopad_result.shape)\n",
    "print(nopad_result.squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8260c57",
   "metadata": {},
   "source": [
    "[(back...)](#Padding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ea0bf8",
   "metadata": {},
   "source": [
    "* ## padding='SAME'\n",
    "![](https://i.imgur.com/vZWnAvN.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5cc3a553",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_result = Conv2D(1, (3, 3), padding='SAME',\n",
    "                    kernel_initializer=kernel_init)(input_img)\n",
    "\n",
    "pad_result = pad_result.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cc9f7057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 6, 6, 1)\n",
      "(1, 6, 6, 1)\n",
      "[[0. 0. 1. 1. 0. 0.]\n",
      " [1. 1. 1. 2. 1. 0.]\n",
      " [0. 2. 1. 2. 2. 1.]\n",
      " [0. 0. 3. 1. 2. 1.]\n",
      " [0. 0. 0. 2. 0. 1.]\n",
      " [0. 0. 0. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(input_img.shape)\n",
    "print(pad_result.shape)\n",
    "print(pad_result.squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858744d4",
   "metadata": {},
   "source": [
    "[(back...)](#Padding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b620f2a4",
   "metadata": {},
   "source": [
    "## ZeroPadding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5ed02013",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_padding = ZeroPadding2D(padding=(1, 1))(input_img)\n",
    "zero_result = Conv2D(1, (3, 3),\n",
    "                     kernel_initializer=kernel_init)(zero_padding)\n",
    "\n",
    "zero_padding = zero_padding.numpy()\n",
    "zero_result = zero_result.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "405ec5fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 6, 6, 1)\n",
      "(1, 8, 8, 1)\n",
      "(1, 6, 6, 1)\n",
      "[[0. 0. 1. 1. 0. 0.]\n",
      " [1. 1. 1. 2. 1. 0.]\n",
      " [0. 2. 1. 2. 2. 1.]\n",
      " [0. 0. 3. 1. 2. 1.]\n",
      " [0. 0. 0. 2. 0. 1.]\n",
      " [0. 0. 0. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(input_img.shape)\n",
    "print(zero_padding.shape)\n",
    "print(zero_result.shape)\n",
    "print(pad_result.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251649d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(zero_result.shape)\n",
    "print(zero_result.squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879cb1b4",
   "metadata": {},
   "source": [
    "[(back...)](#Padding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a49f047",
   "metadata": {},
   "source": [
    "# Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efeb8a3a",
   "metadata": {},
   "source": [
    "\n",
    "* [Average Pooling](#Average-Pooling)\n",
    "* [Max Pooling](#Max-Pooling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e968e8a4",
   "metadata": {},
   "source": [
    "![](https://i.imgur.com/XZQtZC3.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "dc2d5746",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.layers import AveragePooling2D, MaxPool2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "59f98b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = np.array([[1, 2, 2, 0],\n",
    "                      [1, 2, 3, 2],\n",
    "                      [3, 1, 3, 2],\n",
    "                      [0, 2, 0, 2]], dtype='float32').reshape((1, 4, 4, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5c488f",
   "metadata": {},
   "source": [
    "* ## Average Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c52818a",
   "metadata": {},
   "source": [
    "![](https://i.imgur.com/sDKe1To.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "251c25e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_result = AveragePooling2D()(input_img)\n",
    "avg_result = avg_result.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4b98627b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 4, 4, 1)\n",
      "(1, 2, 2, 1)\n",
      "[[1.5  1.75]\n",
      " [1.5  1.75]]\n"
     ]
    }
   ],
   "source": [
    "print(input_img.shape)\n",
    "print(avg_result.shape)\n",
    "print(avg_result.squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc30d85",
   "metadata": {},
   "source": [
    "[(back...)](#Pooling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17a01e4",
   "metadata": {},
   "source": [
    "* ## Max Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42778a5",
   "metadata": {},
   "source": [
    "![](https://i.imgur.com/HZhzUzN.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b54586ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_result = MaxPool2D()(input_img)\n",
    "max_result = max_result.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9051167d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 4, 4, 1)\n",
      "(1, 2, 2, 1)\n",
      "[[2. 3.]\n",
      " [3. 3.]]\n"
     ]
    }
   ],
   "source": [
    "print(input_img.shape)\n",
    "print(max_result.shape)\n",
    "print(max_result.squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2362645",
   "metadata": {},
   "source": [
    "[(back...)](#Pooling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce6c26d",
   "metadata": {},
   "source": [
    "# GlobalPooling\n",
    "\n",
    "* [Global Average Pooling](#Global-Average-Pooling)\n",
    "* [Global Max Pooling](#Global-Max-Pooling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0d477471",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.layers import (GlobalAveragePooling2D,\n",
    "                                     GlobalMaxPooling2D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ea4fca9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = np.load(\"./data/globalpooling_input.npy\")[np.newaxis, ...]\n",
    "input_img = input_img.astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231128f5",
   "metadata": {},
   "source": [
    "* ## Global Average Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c55b39",
   "metadata": {},
   "source": [
    "![](https://i.imgur.com/c62Vie8.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "593cd887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 4, 4, 8)\n",
      "[[[ 0.  1.  2.  3.]\n",
      "  [ 4.  5.  6.  7.]\n",
      "  [ 8.  9. 10. 11.]\n",
      "  [12. 13. 14. 15.]]]\n"
     ]
    }
   ],
   "source": [
    "print(input_img.shape)\n",
    "print(input_img[..., 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d4f21160",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_result = GlobalAveragePooling2D()(input_img)\n",
    "avg_result = avg_result.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1ec7c7e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 8)\n",
      "[ 7.5  7.5  3.5  1.5  0.   1.5 -0.5  0.5]\n"
     ]
    }
   ],
   "source": [
    "print(avg_result.shape)\n",
    "print(avg_result.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "dcdeedcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7.5,  7.5,  3.5,  1.5,  0. ,  1.5, -0.5,  0.5]], dtype=float32)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_img.mean((1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf884a89",
   "metadata": {},
   "source": [
    "[(back...)](#GlobalPooling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53a597e",
   "metadata": {},
   "source": [
    "* ## Global Max Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5d2c4d",
   "metadata": {},
   "source": [
    "![](https://i.imgur.com/XFNnWSe.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c2ad2a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_result = GlobalMaxPooling2D()(input_img)\n",
    "max_result = max_result.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4448b602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 4, 4, 8)\n",
      "[[[ 0.  1.  2.  3.]\n",
      "  [ 4.  5.  6.  7.]\n",
      "  [ 8.  9. 10. 11.]\n",
      "  [12. 13. 14. 15.]]]\n"
     ]
    }
   ],
   "source": [
    "print(input_img.shape)\n",
    "print(input_img[..., 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "48a01a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 8)\n",
      "[15. 15.  7.  3.  0.  3.  0.  2.]\n"
     ]
    }
   ],
   "source": [
    "print(max_result.shape)\n",
    "print(max_result.squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7a7b1f",
   "metadata": {},
   "source": [
    "[(back...)](#GlobalPooling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef06f16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
